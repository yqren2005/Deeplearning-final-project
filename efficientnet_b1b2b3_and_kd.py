# -*- coding: utf-8 -*-
"""EfficientNet b1b2b3 and KD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12_f6e_FwaT_DkCHH8mYT30fUht4UXpcm
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import os
import copy
import time
import random
import numpy as np
import torch
import torch.optim as optim
from torch import nn
from torch.utils.data import Dataset, DataLoader, random_split
import torchvision.transforms as transforms
from torchvision import datasets, models
from tqdm import tqdm_notebook
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

seed = 1024
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)

!pip install efficientnet-pytorch

!pip install scikit-plot

from efficientnet_pytorch import EfficientNet
import scikitplot as skplt

data_dir = '/content/drive/MyDrive/dlproject/data/output/'

train_dir = os.path.join(data_dir, 'train/')
val_dir = os.path.join(data_dir, 'val/')
test_dir = os.path.join(data_dir, 'test/')

classes = ['COVID-19', 'Normal', 'Pneumonia-Bacterial', 'Pneumonia-Viral']
image_size = (300, 300)
batch_size = 16
num_workers = 2

normalize = transforms.Normalize(mean=[0.4907, 0.4907, 0.4908],
                                 std=[0.2219, 0.2219, 0.2219])

image_transforms = {
    "train": transforms.Compose([transforms.Resize(image_size),
                                # transforms.RandomAffine(10, translate=(0.2,0.2), scale=(0.8,1.2)),
                                 transforms.ToTensor(),
                                 normalize]),

    "val": transforms.Compose([transforms.Resize(image_size),
                               transforms.ToTensor(),
                               normalize]),

    "test": transforms.Compose([transforms.Resize(image_size),
                                transforms.ToTensor(),
                                normalize])
}

image_datasets = {
    "train": datasets.ImageFolder(train_dir, image_transforms["train"]),
    "val": datasets.ImageFolder(val_dir, image_transforms["val"]),
    "test": datasets.ImageFolder(test_dir, image_transforms["test"])
}

image_loaders = {
    "train": DataLoader(image_datasets["train"],
                        shuffle=True,
                        batch_size=batch_size,
                        num_workers=num_workers),
    "val": DataLoader(image_datasets["val"],
                      shuffle=True,
                      batch_size=batch_size,
                      num_workers=num_workers),
    "test": DataLoader(image_datasets["test"],
                       shuffle=False,
                       batch_size=batch_size,
                       num_workers=num_workers)
}

dataset_sizes = {k: len(image_datasets[k]) for k in image_datasets}

def train(model, criterion, optimizer, n_epochs, patience, scheduler=None, monitor='val_loss'):
    hist = {'loss':[], 'acc':[], 'val_loss':[], 'val_acc':[]}
    not_improved = 0
    best_weights = None
    val_loss_min = np.Inf
    val_acc_max = -np.Inf

    for epoch in range(n_epochs):
        epoch_stats = []
        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()
            if phase == 'val':
                model.eval()
            losses = 0.0
            corrects = 0
            for data, target in image_loaders[phase]:
                data = data.cuda()
                target = target.cuda()

                out = model(data)
                loss = criterion(out, target)

                if phase == 'train':
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()

                _, preds = torch.max(out, dim=-1)
                losses += loss.item() * data.size(0)
                corrects += torch.sum(preds==target.data).item()

            if phase == 'train' and scheduler:
                scheduler.step()

            epoch_stats += [losses / dataset_sizes[phase],
                            corrects / dataset_sizes[phase]]

        train_loss, train_acc, val_loss, val_acc = epoch_stats
        print('Epoch: {}\tloss: {:.6f}\taccuracy: {:.6f}\tval_loss: {:.6f}\tval_accuracy: {:.6f}'
              .format(epoch+1, train_loss, train_acc, val_loss, val_acc))
        hist['loss'] += [train_loss]
        hist['acc'] += [train_acc]
        hist['val_loss'] += [val_loss]
        hist['val_acc'] += [val_acc]

        if monitor=='val_loss' and val_loss < val_loss_min:
            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'
                  .format(val_loss_min, val_loss))
            best_weights = copy.deepcopy(model.state_dict())
            val_loss_min = val_loss
            not_improved = 0

        elif monitor=='val_acc' and val_acc > val_acc_max:
            print('Validation accuracy increased ({:.6f} --> {:.6f}).  Saving model ...'
                  .format(val_acc_max, val_acc))
            best_weights = copy.deepcopy(model.state_dict())
            val_acc_max = val_acc
            not_improved = 0

        else:
            not_improved += 1
            if not_improved == patience and epoch < n_epochs-1:
                print('Early stopping ...')
                break

    model.load_state_dict(best_weights)
    return model, hist


def test(model, criterion):
    losses = 0.0
    corrects = 0
    num_class = 4
    cm = torch.zeros(num_class, num_class)
    y_pred = torch.FloatTensor().cuda()
    y_pred_proba = torch.FloatTensor().cuda()

    model.eval()
    with torch.no_grad():
        for data, target in image_loaders["test"]:
            data = data.cuda()
            target = target.cuda()

            out = model(data)
            loss = criterion(out, target)

            # update confusion matrix
            _, preds = torch.max(out, dim=-1)
            for t, p in zip(target.view(-1), preds.view(-1)):
                cm[t.long(), p.long()] += 1
            losses += loss.item() * data.size(0)
            corrects += torch.sum(preds==target.data).item()
            y_pred = torch.cat((y_pred, preds), 0)
            proba = torch.softmax(out, dim=1)
            y_pred_proba = torch.cat((y_pred_proba, proba), 0)

    test_loss = losses / dataset_sizes["test"]
    test_acc = corrects / dataset_sizes["test"]
    print("Test loss:", test_loss)
    print("Test accuracy:", test_acc)
    cm_percent = cm / cm.sum(1)
    per_cls_acc = cm_percent.diag().detach().numpy().tolist()
    for i, acc_i in enumerate(per_cls_acc):
        print("Test accuracy of class {}: {:.4f}".format(i, acc_i))
    cm = cm.cpu().numpy()
    y_pred = y_pred.cpu().numpy()
    y_pred_proba = y_pred_proba.cpu().numpy()
    return cm, y_pred, y_pred_proba

def plot_loss_acc(hist, name):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    plt.suptitle(name + ': Accuracy/Loss vs Epoch')
    x = np.arange(1, len(hist['loss'])+1, 1)
    ax1.plot(x, hist['acc'])
    ax1.plot(x, hist['val_acc'])
    ax2.plot(x, hist['loss'])
    ax2.plot(x, hist['val_loss'])
    ax1.set(xlabel='Epoch', ylabel='Accuracy', ylim=(0,1))
    ax2.set(xlabel='Epoch', ylabel='Loss')
    ax1.set_xticks(x)
    ax2.set_xticks(x)
    ax1.legend(['Training', 'Validation'])
    ax2.legend(['Training', 'Validation'])


def plot_confusion_matrix(cm):
    cm = cm.astype('int')
    plt.figure(figsize=(10,8))
    g = sns.heatmap(cm,
                    cmap="YlGnBu",
                    annot=True,
                    fmt="d",
                    xticklabels=classes,
                    yticklabels=classes,
                    square=True,
                    linewidth=1)
    plt.yticks(rotation=0)
    g.xaxis.tick_top()
    g.xaxis.set_label_position('top')

"""###Feature Extractor (B1, input image_size=240, batch_size=32)"""

model = EfficientNet.from_pretrained('efficientnet-b1')

# Freeze model weights
for param in model.parameters():
    param.requires_grad = False

num_ftrs = model._fc.in_features
model._fc = nn.Linear(num_ftrs, 4)

model.cuda()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.001)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.9)
n_epochs = 20
patience = 3
model, hist = train(model, criterion, optimizer, n_epochs, patience, scheduler, monitor='val_acc')
cm, y_pred, y_pred_proba = test(model, criterion)

plot_loss_acc(hist, 'b1')

plot_confusion_matrix(cm)

"""###Finetune"""

model = EfficientNet.from_pretrained('efficientnet-b1', num_classes=4)

model.cuda()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9, weight_decay=0.001)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)
n_epochs = 20
patience = 3
model, hist = train(model, criterion, optimizer, n_epochs, patience, scheduler, monitor='val_acc')
cm, y_pred, y_pred_proba = test(model, criterion)

plot_loss_acc(hist, 'b1')

plot_confusion_matrix(cm)

"""###Finetune + more layers"""

model = EfficientNet.from_pretrained('efficientnet-b1')

num_ftrs = model._fc.in_features
last_layers = nn.Sequential(
                            nn.BatchNorm1d(num_features=num_ftrs),
                            nn.Dropout(0.4),
                            nn.Linear(num_ftrs, 64),
                            nn.ReLU(),
                            nn.BatchNorm1d(num_features=64),
                            nn.Dropout(0.4),
                            nn.Linear(64, 4)
                            )
model._fc = last_layers

model.cuda()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.006, momentum=0.9, weight_decay=0.001)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=6, gamma=0.3)
n_epochs = 40
patience = 5
model, hist = train(model, criterion, optimizer, n_epochs, patience, scheduler, monitor='val_acc')
cm, y_pred, y_pred_proba = test(model, criterion)

plot_loss_acc(hist, 'b1')

plot_confusion_matrix(cm)

y_true = image_datasets['test'].targets
skplt.metrics.plot_roc(y_true, y_pred_proba, figsize=(10,10))

torch.save(model.state_dict(), '/content/drive/MyDrive/dlproject/weights/efficientnet-b1.pt')

print(classification_report(y_true, y_pred))

"""###Feature Extractor (B2: input image_size=260, batch_size=32)"""

model = EfficientNet.from_pretrained('efficientnet-b2')

# Freeze model weights
for param in model.parameters():
    param.requires_grad = False

num_ftrs = model._fc.in_features
model._fc = nn.Linear(num_ftrs, 4)

model.cuda()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.001)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.9)
n_epochs = 20
patience = 3
model, hist = train(model, criterion, optimizer, n_epochs, patience, scheduler)
cm, y_pred, y_pred_proba = test(model, criterion)

plot_loss_acc(hist, 'b2')

plot_confusion_matrix(cm)

"""###Finetune"""

model = EfficientNet.from_pretrained('efficientnet-b2', num_classes=4)

model.cuda()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.002, momentum=0.9, weight_decay=0.001)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)
n_epochs = 20
patience = 3
model, hist = train(model, criterion, optimizer, n_epochs, patience, scheduler)
cm, y_pred, y_pred_proba = test(model, criterion)

plot_loss_acc(hist, 'b2')

plot_confusion_matrix(cm)

"""###Finetune + more layers"""

model = EfficientNet.from_pretrained('efficientnet-b2')

num_ftrs = model._fc.in_features
last_layers = nn.Sequential(
                            nn.BatchNorm1d(num_features=num_ftrs),
                            nn.Dropout(0.3),
                            nn.Linear(num_ftrs, 64),
                            nn.ReLU(),
                            nn.BatchNorm1d(num_features=64),
                            nn.Dropout(0.3),
                            nn.Linear(64, 4)
                            )
model._fc = last_layers

model.cuda()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.004, momentum=0.9, weight_decay=0.001)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)
n_epochs = 40
patience = 10
model, hist = train(model, criterion, optimizer, n_epochs, patience, scheduler, monitor='val_acc')
cm, y_pred, y_pred_proba = test(model, criterion)

plot_loss_acc(hist, 'b2')

plot_confusion_matrix(cm)

skplt.metrics.plot_roc(y_true, y_pred_proba, figsize=(10,10))

"""###Feature Extractor (B3, input image_size=300, batch_size=16)"""

model = EfficientNet.from_pretrained('efficientnet-b3')

# Freeze model weights
for param in model.parameters():
    param.requires_grad = False

num_ftrs = model._fc.in_features
model._fc = nn.Linear(num_ftrs, 4)

model.cuda()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.001)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.9)
n_epochs = 20
patience = 3
model, hist = train(model, criterion, optimizer, n_epochs, patience, scheduler)
cm, y_pred, y_pred_proba = test(model, criterion)

plot_loss_acc(hist, 'b3')

plot_confusion_matrix(cm)

"""###Finetune"""

model = EfficientNet.from_pretrained('efficientnet-b3', num_classes=4)

model.cuda()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.002, momentum=0.9, weight_decay=0.001)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)
n_epochs = 20
patience = 3
model, hist = train(model, criterion, optimizer, n_epochs, patience, scheduler)
cm, y_pred, y_pred_proba = test(model, criterion)

plot_loss_acc(hist, 'b3')

plot_confusion_matrix(cm)

"""###Finetune + more layers (Best result)"""

model = EfficientNet.from_pretrained('efficientnet-b3')

num_ftrs = model._fc.in_features
last_layers = nn.Sequential(
                            # nn.BatchNorm1d(num_features=num_ftrs),
                            # nn.Dropout(0.5),
                            # nn.Linear(num_ftrs, 64),
                            # nn.ReLU(),
                            nn.BatchNorm1d(num_features=num_ftrs),
                            nn.Dropout(0.3),
                            nn.Linear(num_ftrs, 4)
                            )
model._fc = last_layers

model.cuda()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.002, momentum=0.9, weight_decay=0.001)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)
n_epochs = 40
patience = 10
model, hist = train(model, criterion, optimizer, n_epochs, patience, scheduler, monitor='val_acc')
cm, y_pred, y_pred_proba = test(model, criterion)

plot_loss_acc(hist, 'b3')

plot_confusion_matrix(cm)

y_true = image_datasets['test'].targets
for i in range(4):
    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba[:,i], pos_label=i)
    print('AUC of Class {}: {:.6f}'.format(i, auc(fpr, tpr)))

y_true = image_datasets['test'].targets
skplt.metrics.plot_roc(y_true, y_pred_proba, figsize=(10,10))

torch.save(model.state_dict(), '/content/drive/MyDrive/dlproject/weights/efficientnet-b3.pt')

print(classification_report(y_true, y_pred))

import pandas as pd

acc = {'B0':[0.81290672451193, 0.836225596529284, 0.879067245119305],
       'B1':[0.832971800433839, 0.857917570498915, 0.888828633405639],
       'B2':[0.815075921908893, 0.848156182212581, 0.892082429501084],
       'B3':[0.809652928416486, 0.866594360086767, 0.895336225596529]
       }

df = pd.DataFrame(acc, index=['Feature Extracor', 'Finetune', 'Finetune+layers'])
df.plot.bar(figsize=(10,6))
plt.xticks(rotation=0)
plt.title('Overall Test Accuracy')
plt.ylim([0,1])
plt.legend(bbox_to_anchor=(1.01, 1.0), loc='upper left')
plt.show()

# Test accuracy of class 0: 0.9844 B3
# Test accuracy of class 1: 0.9771
# Test accuracy of class 2: 0.8602
# Test accuracy of class 3: 0.7289

# Test accuracy of class 0: 0.9805 B2
# Test accuracy of class 1: 0.9725
# Test accuracy of class 2: 0.8502
# Test accuracy of class 3: 0.7410

# Test accuracy of class 0: 0.9728 B1
# Test accuracy of class 1: 0.9587
# Test accuracy of class 2: 0.8819
# Test accuracy of class 3: 0.6988

# Test accuracy of class 0: 0.9728 B0
# Test accuracy of class 1: 0.9572
# Test accuracy of class 2: 0.8686
# Test accuracy of class 3: 0.6717

per_cls_acc = {'Class 0':[0.9728,0.9728,0.9805,0.9844],
               'Class 1':[0.9572,0.9587,0.9725,0.9771],
               'Class 2':[0.8686,0.8819,0.8502,0.8602],
               'Class 3':[0.6717,0.6988,0.7410,0.7289]}

nets = ['B0', 'B1', 'B2', 'B3']
df = pd.DataFrame(per_cls_acc, index=nets)
df.plot.bar(figsize=(10,6))
plt.xticks(rotation=0)
plt.title('Per Class Test Accuracy for Finetune+layers')
plt.ylim([0,1])
plt.legend(bbox_to_anchor=(1.01, 1.0), loc='upper left')
plt.show()

"""###Knowledge Distillation"""

class LeNet(nn.Module):
    def __init__(self):
        super(LeNet, self).__init__()
        # 3 input image channel, 6 output channels, 5x5 square conv kernel
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 53 * 53, 84) # 53x53 image dimension
        self.fc2 = nn.Linear(84, 4)

    def forward(self, x):
        x = nn.functional.interpolate(x, size=224) # resize input to 224
        x = torch.max_pool2d(torch.relu(self.conv1(x)), (2, 2))
        x = torch.max_pool2d(torch.relu(self.conv2(x)), (2, 2))
        x = x.view(-1, int(x.nelement() / x.shape[0]))
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

class DistillationLoss(nn.Module):

    def forward(self, student_output, teacher_output, T):
        distill_loss = None
        P = nn.functional.softmax(teacher_output / T, dim=1)
        Q = nn.functional.log_softmax(student_output / T, dim=1)
        distill_loss = nn.KLDivLoss(reduction='batchmean')(Q, P)
        # distill_loss = -(P * Q).sum(dim=1).mean()
        return distill_loss * T**2


def distill(student, teacher, optimizer, n_epochs, patience, alpha, T, scheduler=None, monitor='val_loss'):
    hist = {'loss':[], 'acc':[], 'val_loss':[], 'val_acc':[]}
    not_improved = 0
    best_weights = None
    val_loss_min = np.Inf
    val_acc_max = -np.Inf

    teacher.eval()
    criterion_student = nn.CrossEntropyLoss()
    criterion_distill = DistillationLoss()

    for epoch in range(n_epochs):
        epoch_stats = []
        for phase in ['train', 'val']:
            if phase == 'train':
                student.train()
            if phase == 'val':
                student.eval()
            losses = 0.0
            corrects = 0
            for data, target in image_loaders[phase]:
                data = data.cuda()
                target = target.cuda()

                out = student(data)
                with torch.no_grad():
                    teacher_output = teacher(data)
                loss_distill = criterion_distill(out, teacher_output, T)
                loss_student = criterion_student(out, target)
                loss = alpha * loss_student + (1 - alpha) * loss_distill

                if phase == 'train':
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()

                _, preds = torch.max(out, dim=-1)
                losses += loss.item() * data.size(0)
                corrects += torch.sum(preds==target.data).item()

            if phase == 'train' and scheduler:
                scheduler.step()

            epoch_stats += [losses / dataset_sizes[phase],
                            corrects / dataset_sizes[phase]]

        train_loss, train_acc, val_loss, val_acc = epoch_stats
        print('Epoch: {}\tloss: {:.6f}\taccuracy: {:.6f}\tval_loss: {:.6f}\tval_accuracy: {:.6f}'
              .format(epoch+1, train_loss, train_acc, val_loss, val_acc))
        hist['loss'] += [train_loss]
        hist['acc'] += [train_acc]
        hist['val_loss'] += [val_loss]
        hist['val_acc'] += [val_acc]

        if monitor=='val_loss' and val_loss < val_loss_min:
            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'
                  .format(val_loss_min, val_loss))
            best_weights = copy.deepcopy(student.state_dict())
            val_loss_min = val_loss
            not_improved = 0

        elif monitor=='val_acc' and val_acc > val_acc_max:
            print('Validation accuracy increased ({:.6f} --> {:.6f}).  Saving model ...'
                  .format(val_acc_max, val_acc))
            best_weights = copy.deepcopy(student.state_dict())
            val_acc_max = val_acc
            not_improved = 0

        else:
            not_improved += 1
            if not_improved == patience and epoch < n_epochs-1:
                print('Early stopping ...')
                break

    student.load_state_dict(best_weights)
    return student, hist

"""###KD for LeNet"""

# Train student LeNet without KD
student = LeNet().cuda()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(student.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)
n_epochs = 10
patience = 3
student, hist = train(student, criterion, optimizer, n_epochs, patience, monitor='val_acc')
cm, y_pred, y_pred_proba = test(student, criterion)

plot_loss_acc(hist, 'student')

# Load trained teacher B3
model = EfficientNet.from_pretrained('efficientnet-b3')

num_ftrs = model._fc.in_features
last_layers = nn.Sequential(
                            nn.BatchNorm1d(num_features=num_ftrs),
                            nn.Dropout(0.3),
                            nn.Linear(num_ftrs, 4)
                            )
model._fc = last_layers

model.load_state_dict(torch.load('/content/drive/MyDrive/dlproject/weights/efficientnet-b3.pt'))
model.cuda()
model.eval()

# Train student with KD
student = LeNet().cuda()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(student.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)
n_epochs = 10
patience = 3
alpha = 0.1
T = 3
student, hist = distill(student, model, optimizer, n_epochs, patience, alpha, T, monitor='val_acc')
cm, y_pred, y_pred_proba = test(student, criterion)

plot_loss_acc(hist, 'student')

"""###KD for B0-3"""

names = ['efficientnet-b0','efficientnet-b1','efficientnet-b2','efficientnet-b3']

for name in names:
    print('\nTrain student {} (Feature extractor) without KD ..'.format(name))
    student = EfficientNet.from_pretrained(name)

    # Freeze model weights
    for param in student.parameters():
        param.requires_grad = False

    num_ftrs = student._fc.in_features
    student._fc = nn.Linear(num_ftrs, 4)
    student.cuda()

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(student.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)
    n_epochs = 10
    patience = 3
    student, hist = train(student, criterion, optimizer, n_epochs, patience, monitor='val_acc')
    cm, y_pred, y_pred_proba = test(student, criterion)

    print('\nLoad trained teacher B3')
    model = EfficientNet.from_pretrained('efficientnet-b3')

    num_ftrs = model._fc.in_features
    last_layers = nn.Sequential(
                                nn.BatchNorm1d(num_features=num_ftrs),
                                nn.Dropout(0.3),
                                nn.Linear(num_ftrs, 4)
                                )
    model._fc = last_layers

    model.load_state_dict(torch.load('/content/drive/MyDrive/dlproject/weights/efficientnet-b3.pt'))
    model.cuda()
    model.eval()

    print('\nTrain student {} (Feature extractor) with KD ..'.format(name))
    student = EfficientNet.from_pretrained(name)

    # Freeze model weights
    for param in student.parameters():
        param.requires_grad = False

    num_ftrs = student._fc.in_features
    student._fc = nn.Linear(num_ftrs, 4)
    student.cuda()

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(student.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)
    n_epochs = 10
    patience = 3
    alpha = 0.1
    T = 3
    student, hist = distill(student, model, optimizer, n_epochs, patience, alpha, T, monitor='val_acc')
    cm, y_pred, y_pred_proba = test(student, criterion)

names = ['efficientnet-b0','efficientnet-b1','efficientnet-b2','efficientnet-b3']

for name in names:
    print('\nTrain student {} (Finetune) without KD ..'.format(name))
    student = EfficientNet.from_pretrained(name, num_classes=4)
    student.cuda()

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(student.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)
    n_epochs = 10
    patience = 3
    student, hist = train(student, criterion, optimizer, n_epochs, patience, monitor='val_acc')
    cm, y_pred, y_pred_proba = test(student, criterion)

    print('\nLoad trained teacher B3')
    model = EfficientNet.from_pretrained('efficientnet-b3')

    num_ftrs = model._fc.in_features
    last_layers = nn.Sequential(
                                nn.BatchNorm1d(num_features=num_ftrs),
                                nn.Dropout(0.3),
                                nn.Linear(num_ftrs, 4)
                                )
    model._fc = last_layers

    model.load_state_dict(torch.load('/content/drive/MyDrive/dlproject/weights/efficientnet-b3.pt'))
    model.cuda()
    model.eval()

    print('\nTrain student {} (Finetune) with KD ..'.format(name))
    student = EfficientNet.from_pretrained(name, num_classes=4)
    student.cuda()

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(student.parameters(), lr=0.001, momentum=0.9, weight_decay=0.001)
    n_epochs = 10
    patience = 3
    alpha = 0.1
    T = 3
    student, hist = distill(student, model, optimizer, n_epochs, patience, alpha, T, monitor='val_acc')
    cm, y_pred, y_pred_proba = test(student, criterion)

